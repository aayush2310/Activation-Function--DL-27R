If we dont apply activation function to our neural network then it wont be able to capture non-linear transformation/data.
Ideal Activation Function:-
1)Non linear activation function
2)differentiable(back propagation)
3)computationally inexpensive
4)zero centered/normalized-converges faster on normalized data(eg-tanh)
5)Non-saturating(the o/p should not saturate at a point)for saturating function-vanishing gradient problem

activation functions:-
1)Sigmoid
     advantages:-
           i)o/p b/w 0-1=>can be treated as probability in o/p layer.It is treated as probability while dealing with binary classification problem.
           ii)Non-linear function
           iii)differentiable
     disadvantages:-
            i)saturating function-so,sigmoid is not used in hidden layer-vanishing gradient problem
            ii)Non-zero centered-not normalized-training gets slow-slow convergence
            iii)computationally expensive
            
            
2)Tanh
       advantages:-
                1)non-linear
                2)differentiable
                3)zero-centered
       disadvantages:-
                1)saturating function
                2)computationally expensive
                
 3)Relu
         advantages:-
                    1)non-linear
                    2)non-saturated in +ve region
                    3)computationally inexpensive
                    4)faster convergence
          disadvantage:-
                    1)non-differentiable(apply if else in coding)
                    2)not zero centered(to do so we use batch normalization=>normalizes the o/p of the hidden layers)
                    3)dying relu problem
